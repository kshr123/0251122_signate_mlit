# MLflow 使い方ガイド

> **このドキュメントについて**: MLflowが初めての方向けに、基本的な使い方と実験結果の見方を説明します。

---

## 📋 目次

1. [MLflowとは](#1-mlflowとは)
2. [基本的な使い方](#2-基本的な使い方)
3. [MLflow UI の使い方](#3-mlflow-ui-の使い方)
4. [実験結果の見方](#4-実験結果の見方)
5. [よくある質問](#5-よくある質問)

---

## 1. MLflowとは

### 1.1 概要

**MLflow** = 機械学習の実験を記録・管理するツール

- 🎯 **目的**: 「どの実験が良かったか」を後から振り返れるようにする
- 📊 **記録内容**: パラメータ、評価スコア、モデルファイル、提出ファイル等
- 🔍 **メリット**: 複数の実験を比較して、何が効果的だったか分析できる

### 1.2 このプロジェクトでの使い方

```
実験実行 → MLflowが自動記録 → MLflow UIで確認
```

**記録されるもの**:
- パラメータ: シード値、特徴量数、モデル設定
- メトリクス: CVスコア（平均、標準偏差、Fold別）
- アーティファクト: 提出ファイル、モデルファイル
- タグ: 実験の種類、ステータス、メモ

---

## 2. 基本的な使い方

### 2.1 実験の実行

ベースラインモデルを訓練すると、自動でMLflowに記録されます。

```bash
# ベースライン訓練（MLflowが自動記録）
python 04_src/training/train_baseline.py
```

**実行すると表示される情報**:
```
✅ Run completed: abc123def456
📊 CV RMSE: 12345.6789 ± 123.45
📁 Submission: 06_submissions/submission_20251124_143022.csv
```

- `Run ID`: この実験の一意識別子（`abc123def456`）
- `CV RMSE`: クロスバリデーションのスコア
- `Submission`: 提出ファイルのパス

### 2.2 MLflow UI の起動

実験結果を見るためにMLflow UIを起動します。

```bash
# プロジェクトルートで実行
mlflow ui --port 5000
```

**起動後**:
1. ブラウザが自動で開く（開かない場合は手動で http://localhost:5000 にアクセス）
2. 左側に実験一覧が表示される
3. 右側に実験の詳細が表示される

**停止方法**: ターミナルで `Ctrl + C`

---

## 3. MLflow UI の使い方

### 3.1 画面の構成

```
┌─────────────────────────────────────────────────────┐
│  MLflow                                             │
├──────────────┬──────────────────────────────────────┤
│              │  Run 一覧                            │
│  Experiments │  ┌────────────────────────────────┐ │
│              │  │ baseline_20251124_143022      │ │
│  ├ signate_  │  │ CV RMSE: 12345.6789          │ │
│  │  mlit_    │  │ Status: completed             │ │
│  │  rental_  │  └────────────────────────────────┘ │
│  │  price    │  ┌────────────────────────────────┐ │
│              │  │ baseline_20251124_120000      │ │
│  ├ test_     │  │ CV RMSE: 12456.7890          │ │
│    experiment│  │ Status: completed             │ │
│              │  └────────────────────────────────┘ │
└──────────────┴──────────────────────────────────────┘
```

### 3.2 実験一覧の見方

#### カラムの意味

| カラム名 | 説明 | 例 |
|---------|------|-----|
| Run Name | 実験の名前 | `baseline_20251124_143022` |
| Start Time | 実験開始時刻 | `2025-11-24 14:30:22` |
| User | 実行したユーザー | `kotaro` |
| Status | 実験のステータス | `FINISHED`, `RUNNING`, `FAILED` |
| cv_rmse_mean | CVスコア平均 | `12345.6789` |

#### ソート・フィルタ

**スコアでソート**:
1. `cv_rmse_mean` カラムのヘッダーをクリック
2. 昇順（小さい順）で良いスコアが上に表示される

**タグでフィルタ**:
1. 検索ボックスに `tags.experiment_type = "baseline"` と入力
2. ベースラインの実験のみ表示される

### 3.3 実験の詳細を見る

#### 手順

1. 実験一覧から見たい実験をクリック
2. 詳細画面が開く

#### タブの説明

**① Overview タブ**
- 実験の概要（Run ID、開始時刻、ステータス）
- タグ（experiment_type, status, note等）

**② Parameters タブ**
- 記録されたパラメータ一覧
- 例: `seed=42`, `learning_rate=0.05`, `n_features=50`

**③ Metrics タブ**
- 記録されたメトリクス一覧
- 例: `cv_rmse_mean=12345.6789`, `train_size=363924`

**④ Artifacts タブ**
- 保存されたファイル一覧
- 提出ファイル（`submission_*.csv`）
- モデルファイル（`model/`）
- 特徴量リスト（`features.txt`）

---

## 4. 実験結果の見方

### 4.1 単一実験の確認

**目的**: 1つの実験の結果を詳しく見る

#### 確認項目

**① CVスコア**
```
Metrics タブ:
- cv_rmse_mean: 12345.6789  ← 平均スコア（小さいほど良い）
- cv_rmse_std:  123.45      ← 標準偏差（小さいほど安定）
- cv_rmse_fold_0: 12300.00  ← Fold 0のスコア
- cv_rmse_fold_1: 12400.00  ← Fold 1のスコア
- ...
```

**判断基準**:
- ✅ **良い実験**: cv_rmse_mean が小さく、cv_rmse_std も小さい
- ⚠️ **不安定**: cv_rmse_std が大きい → Fold間でバラつきあり
- ❌ **過学習の疑い**: Fold間で大きな差がある

**② 使用した特徴量**
```
Parameters タブ:
- n_features: 50           ← 特徴量の数
- feature_cols: [...]      ← 使用した特徴量リスト

Artifacts タブ:
- features.txt をダウンロード → 特徴量リストを確認
```

**③ 提出ファイル**
```
Artifacts タブ:
- submission_20251124_143022.csv をダウンロード
- このファイルを提出すると、この実験のスコアが再現できる
```

### 4.2 複数実験の比較

**目的**: どの実験が良かったか比較する

#### 手順

1. **比較したい実験にチェック**
   - 実験一覧で2つ以上の実験にチェックを入れる

2. **Compare ボタンをクリック**
   - 画面上部の `Compare` ボタンを押す

3. **比較画面が表示される**

#### 比較画面の見方

**① Parallel Coordinates Plot**
```
各実験がどのパラメータでどのスコアだったか視覚的に表示
→ どのパラメータが効いているか分かる
```

**② Scatter Plot**
```
横軸: パラメータ（例: learning_rate）
縦軸: メトリクス（例: cv_rmse_mean）
→ パラメータとスコアの関係が分かる
```

**③ Table View**
```
┌────────────────┬──────────┬──────────────┬───────────────┐
│ Run Name       │ seed     │ n_features   │ cv_rmse_mean  │
├────────────────┼──────────┼──────────────┼───────────────┤
│ baseline_v1    │ 42       │ 50           │ 12345.6789    │
│ baseline_v2    │ 42       │ 60           │ 12200.5678    │ ← 良い
│ baseline_v3    │ 30       │ 50           │ 12400.1234    │
└────────────────┴──────────┴──────────────┴───────────────┘
```

### 4.3 ベストモデルの特定

#### 方法1: スコアでソート

```
1. 実験一覧で cv_rmse_mean カラムをクリック（昇順ソート）
2. 一番上の実験 = ベストモデル
3. その実験をクリックして詳細確認
```

#### 方法2: 検索API（上級者向け）

```python
import mlflow

# ベストスコアのRunを取得
best_run = mlflow.search_runs(
    order_by=["metrics.cv_rmse_mean ASC"],
    max_results=1
)

print(best_run[["run_id", "metrics.cv_rmse_mean"]])
```

---

## 5. よくある質問

### Q1. MLflow UIが起動できません

**A**: 以下を確認してください

1. **ポート5000が使用中の場合**
   ```bash
   # 別のポートを指定
   mlflow ui --port 5001
   ```

2. **mlrunsディレクトリがない場合**
   ```bash
   # プロジェクトルートで実行していますか？
   pwd
   # /Users/kotaro/Desktop/ML/20251122_signamte_mlit であるべき
   ```

### Q2. 実験が表示されません

**A**: 以下を確認してください

1. **実験が正常に完了していますか？**
   ```bash
   # 訓練スクリプトを実行
   python 04_src/training/train_baseline.py

   # ✅ Run completed: ... と表示されるか確認
   ```

2. **正しい実験を選択していますか？**
   ```
   左側の Experiments から "signate_mlit_rental_price" を選択
   ```

### Q3. Run IDとは何ですか？

**A**: 各実験の一意識別子です

```
Run ID = abc123def456  （ランダムな文字列）

用途:
- 実験を一意に特定する
- 後から同じ実験を参照する
- 提出ファイルとの紐付け
```

### Q4. 記録されたデータはどこに保存されますか？

**A**: `mlruns/` ディレクトリに保存されます

```
mlruns/
├── 0/                    # デフォルト実験
├── 123456789/            # 実験ID（signate_mlit_rental_price）
│   ├── abc123def456/     # Run ID
│   │   ├── artifacts/    # アーティファクト
│   │   │   ├── submission_*.csv
│   │   │   └── model/
│   │   ├── metrics/      # メトリクス
│   │   ├── params/       # パラメータ
│   │   └── tags/         # タグ
│   └── ...
└── models/
```

**注意**: `mlruns/` は `.gitignore` に含まれているため、Gitには記録されません

### Q5. 過去の実験を削除できますか？

**A**: できますが、推奨しません

```bash
# UIから削除（非推奨）
# 実験の詳細画面 → Delete ボタン

# ディレクトリごと削除（非推奨）
rm -rf mlruns/
```

**推奨**: 削除せず、タグで管理する
```python
# 失敗した実験にタグを付ける
mlflow.set_tag("status", "failed")
mlflow.set_tag("note", "過学習のため破棄")
```

### Q6. チームで実験結果を共有したい

**A**: このプロジェクトではローカルのみですが、将来的には以下の方法があります

**方法1**: リモートトラッキングサーバー（本格的）
```bash
# サーバー起動（共有マシン）
mlflow server --host 0.0.0.0 --port 5000

# クライアント設定
export MLFLOW_TRACKING_URI=http://server-ip:5000
```

**方法2**: ファイル共有（簡易的）
```bash
# mlrunsディレクトリを圧縮
tar -czf mlruns.tar.gz mlruns/

# チームメンバーに共有
# → 解凍すればMLflow UIで確認可能
```

---

## 🎯 まとめ

### 基本フロー

```
1. 実験実行
   python 04_src/training/train_baseline.py

2. MLflow UI起動
   mlflow ui --port 5000

3. ブラウザで確認
   http://localhost:5000

4. 実験を比較
   - スコアでソート
   - 複数選択して Compare

5. ベストモデル特定
   - 提出ファイルをダウンロード
   - コンペに提出
```

### 覚えておくべきこと

✅ **Run ID** = 実験の識別子
✅ **cv_rmse_mean** = CVスコア平均（小さいほど良い）
✅ **Artifacts** = 提出ファイルやモデルファイル
✅ **Compare** = 複数実験を比較する機能

---

**最終更新**: 2025-11-24
