"""
exp003_baseline_v2 è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç‰¹å¾´é‡ã‚’å³é¸ã—ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ‰‹æ³•ã‚’é©ç”¨ã—ãŸæ–°ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã€‚
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: addr1_1, addr1_2, bukken_type, land_youto, land_toshi
- ãƒ©ãƒ™ãƒ« + ã‚«ã‚¦ãƒ³ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: 11ã‚«ãƒ©ãƒ 
- year_builtå¤‰æ›ã€money_sonotaé›†ç´„
"""

import sys
from pathlib import Path

# ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å„ªå…ˆ
current_dir = Path(__file__).resolve().parent
sys.path.insert(0, str(current_dir))

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).resolve().parents[3]
sys.path.insert(1, str(project_root / "04_src"))

import json
import mlflow
import polars as pl
import lightgbm as lgb
import numpy as np
import yaml
from datetime import datetime
from sklearn.model_selection import KFold

# å…±é€šã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆ04_src/ï¼‰
from data.loader import DataLoader
from features.base import set_seed
from evaluation.metrics import calculate_mape
from training.utils.mlflow_helper import (
    log_dataset_info,
    log_cv_results,
    log_feature_list,
    log_model_params,
)

# å®Ÿé¨“å›ºæœ‰ã®å‰å‡¦ç†ï¼ˆã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ï¼‰
from preprocessing import (
    preprocess_for_training,
    ALL_FEATURES,
    CATEGORICAL_FEATURES,
    TARGET_ENCODING_COLUMNS,
    COUNT_ENCODING_COLUMNS,
    NUMERIC_FEATURES,
)


# ===== è¨­å®š =====
SEED = 42
N_SPLITS = 3
NUM_BOOST_ROUND = 100


def train_baseline_v2():
    """exp003 baseline_v2 ã®è¨“ç·´"""

    # ã‚·ãƒ¼ãƒ‰å›ºå®š
    set_seed(SEED)

    # MLflowå®Ÿé¨“è¨­å®š
    mlflow.set_experiment("signate_mlit_rental_price")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"baseline_v2_{timestamp}"

    with mlflow.start_run(run_name=run_name):
        print(f"ğŸš€ è¨“ç·´é–‹å§‹: {run_name}")

        # ===== ã‚¿ã‚°è¨­å®š =====
        mlflow.set_tag("experiment_type", "baseline_v2")
        mlflow.set_tag("experiment_id", "exp003")
        mlflow.set_tag("model_family", "gbdt")
        mlflow.set_tag("status", "running")

        # ===== ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ² =====
        mlflow.log_param("seed", SEED)
        mlflow.log_param("model_type", "LightGBM")
        mlflow.log_param("n_splits", N_SPLITS)
        mlflow.log_param("num_boost_round", NUM_BOOST_ROUND)
        mlflow.log_param("n_target_encoding_columns", len(TARGET_ENCODING_COLUMNS))
        mlflow.log_param("n_count_encoding_columns", len(COUNT_ENCODING_COLUMNS))
        mlflow.log_param("n_numeric_features", len(NUMERIC_FEATURES))

        # ===== ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ =====
        print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        config_path = project_root / "03_configs" / "data.yaml"
        with open(config_path, "r", encoding="utf-8") as f:
            data_config = yaml.safe_load(f)

        # ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
        data_config["data"]["train_path"] = str(project_root / data_config["data"]["train_path"])
        data_config["data"]["test_path"] = str(project_root / data_config["data"]["test_path"])
        data_config["data"]["sample_submit_path"] = str(project_root / data_config["data"]["sample_submit_path"])

        loader = DataLoader(config=data_config, add_address_columns=False)

        train = loader.load_train()
        test = loader.load_test()

        # IDåˆ—ã‚’å…ˆã«ä¿å­˜ï¼ˆå‰å‡¦ç†å¾Œã«å‚ç…§ã™ã‚‹ãŸã‚ï¼‰
        train_ids = np.arange(len(train))
        test_ids = test["id"].to_numpy()

        print(f"  - Train: {train.shape}")
        print(f"  - Test: {test.shape}")

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¨˜éŒ²
        log_dataset_info(train, prefix="train")
        log_dataset_info(test, prefix="test")

        # ===== CVåˆ†å‰²ã‚’å…ˆã«ä½œæˆï¼ˆTargetEncodingç”¨ï¼‰ =====
        cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)
        cv_splits = list(cv.split(train))

        # ===== å‰å‡¦ç† =====
        print("\nğŸ”§ å‰å‡¦ç†ä¸­...")
        X_train, X_test, y_train = preprocess_for_training(train, test, cv_splits=cv_splits)

        print(f"\nğŸ“Š ç‰¹å¾´é‡æƒ…å ±:")
        print(f"  - ç‰¹å¾´é‡æ•°: {len(ALL_FEATURES)}")
        print(f"  - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡: {len(CATEGORICAL_FEATURES)}")

        # ç‰¹å¾´é‡æƒ…å ±ã‚’è¨˜éŒ²
        mlflow.log_param("n_features", len(ALL_FEATURES))
        mlflow.log_param("n_categorical_features", len(CATEGORICAL_FEATURES))

        # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’ä¿å­˜
        log_feature_list(ALL_FEATURES, artifact_path="features.txt")

        # ===== LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š =====
        lgb_params = {
            "objective": "regression",
            "metric": "mape",
            "learning_rate": 0.05,
            "num_leaves": 31,
            "seed": SEED,
            "verbose": -1,
            "force_row_wise": True,
        }

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²
        log_model_params(lgb_params, prefix="lgb")

        # ===== ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆ3-Fold CVï¼‰ =====
        print("\nğŸ¤– 3-Fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹...")

        cv_scores = []
        oof_predictions = np.zeros(len(X_train))
        test_predictions = np.zeros(len(X_test))
        feature_importance = np.zeros(len(ALL_FEATURES))

        # Polars â†’ NumPyå¤‰æ›
        X_train_np = X_train.to_numpy()
        X_test_np = X_test.to_numpy()
        y_train_np = y_train.to_numpy()

        for fold_idx, (train_idx, val_idx) in enumerate(cv_splits):
            print(f"\n--- Fold {fold_idx + 1}/{N_SPLITS} ---")

            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            X_tr, X_val = X_train_np[train_idx], X_train_np[val_idx]
            y_tr, y_val = y_train_np[train_idx], y_train_np[val_idx]

            # LightGBM Datasetä½œæˆ
            cat_features = CATEGORICAL_FEATURES if CATEGORICAL_FEATURES else "auto"
            train_data = lgb.Dataset(
                X_tr,
                label=y_tr,
                feature_name=ALL_FEATURES,
                categorical_feature=cat_features,
            )

            val_data = lgb.Dataset(
                X_val,
                label=y_val,
                feature_name=ALL_FEATURES,
                categorical_feature=cat_features,
                reference=train_data,
            )

            # è¨“ç·´
            model = lgb.train(
                lgb_params,
                train_data,
                num_boost_round=NUM_BOOST_ROUND,
                valid_sets=[train_data, val_data],
                valid_names=["train", "valid"],
            )

            # äºˆæ¸¬
            val_pred = model.predict(X_val)
            oof_predictions[val_idx] = val_pred

            # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ï¼‰
            test_pred = model.predict(X_test_np)
            test_predictions += test_pred / N_SPLITS

            # MAPEè¨ˆç®—
            mape_score = calculate_mape(y_val, val_pred)
            cv_scores.append(mape_score)

            # ç‰¹å¾´é‡é‡è¦åº¦ã‚’è“„ç©ï¼ˆgainï¼‰
            feature_importance += model.feature_importance(importance_type="gain") / N_SPLITS

            print(f"  Validation MAPE: {mape_score:.4f}%")

        # ===== CVçµæœã¾ã¨ã‚ =====
        print("\n" + "=" * 60)
        print("ğŸ“ˆ ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ")
        print("=" * 60)
        print(f"  å¹³å‡ MAPE: {np.mean(cv_scores):.4f}%")
        print(f"  æ¨™æº–åå·®:   {np.std(cv_scores):.4f}%")
        print(f"  æœ€å°å€¤:     {np.min(cv_scores):.4f}%")
        print(f"  æœ€å¤§å€¤:     {np.max(cv_scores):.4f}%")
        print("=" * 60)

        # CVçµæœã‚’MLflowã«è¨˜éŒ²
        log_cv_results(np.array(cv_scores), metric_name="mape")

        # OOF MAPE
        oof_mape = calculate_mape(y_train_np, oof_predictions)
        mlflow.log_metric("oof_mape", oof_mape)
        print(f"\n  OOF MAPE: {oof_mape:.4f}%")

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path(__file__).parent.parent / "outputs"
        output_dir.mkdir(parents=True, exist_ok=True)

        # ===== OOFäºˆæ¸¬ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼åˆ†æç”¨ï¼‰ =====
        print("\nğŸ“Š OOFäºˆæ¸¬ä¿å­˜ä¸­...")
        oof_df = pl.DataFrame({
            "id": train_ids,
            "actual": y_train_np,
            "predicted": oof_predictions,
        })
        oof_path = output_dir / f"oof_predictions_{timestamp}.csv"
        oof_df.write_csv(oof_path)
        print(f"  âœ“ ä¿å­˜å®Œäº†: {oof_path}")
        mlflow.log_artifact(oof_path, artifact_path="predictions")

        # ===== ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜ =====
        print("\nğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜ä¸­...")
        importance_dict = {
            "feature": ALL_FEATURES,
            "importance": feature_importance.tolist(),
        }
        # ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’è¡¨ç¤º
        sorted_indices = np.argsort(feature_importance)[::-1]
        print("  Top 10 Features:")
        for i, idx in enumerate(sorted_indices[:10]):
            print(f"    {i+1}. {ALL_FEATURES[idx]}: {feature_importance[idx]:.4f}")

        # JSONå½¢å¼ã§ä¿å­˜
        importance_path = output_dir / f"feature_importance_{timestamp}.json"
        with open(importance_path, "w", encoding="utf-8") as f:
            json.dump(importance_dict, f, ensure_ascii=False, indent=2)
        print(f"  âœ“ ä¿å­˜å®Œäº†: {importance_path}")
        mlflow.log_artifact(importance_path, artifact_path="feature_importance")

        # CSVå½¢å¼ã§ã‚‚ä¿å­˜ï¼ˆå¯è¦–åŒ–ã—ã‚„ã™ã„ï¼‰
        importance_df = pl.DataFrame({
            "feature": ALL_FEATURES,
            "importance": feature_importance,
        }).sort("importance", descending=True)
        importance_csv_path = output_dir / f"feature_importance_{timestamp}.csv"
        importance_df.write_csv(importance_csv_path)
        mlflow.log_artifact(importance_csv_path, artifact_path="feature_importance")

        # ===== æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ =====
        print("\nğŸ“¤ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆä¸­...")

        submission = pl.DataFrame({
            "id": test_ids,
            "money_room": test_predictions,
        })

        submission_path = output_dir / f"submission_{timestamp}.csv"
        submission.write_csv(submission_path)

        print(f"  âœ“ ä¿å­˜å®Œäº†: {submission_path}")

        # MLflowã«è¨˜éŒ²
        mlflow.log_artifact(submission_path, artifact_path="submissions")

        # ===== å®Ÿé¨“å®Œäº† =====
        mlflow.set_tag("status", "completed")
        print("\nâœ… è¨“ç·´å®Œäº†ï¼")


if __name__ == "__main__":
    train_baseline_v2()
