"""
exp004_age_features è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

exp003ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€ç¯‰å¹´æ•°é–¢é€£ç‰¹å¾´é‡ã‚’è¿½åŠ :
1. building_age: ç¯‰å¹´æ•°ï¼ˆ2024 - year_builtï¼‰
2. building_age_bin: ç¯‰å¹´æ•°5å¹´å˜ä½ã‚«ãƒ†ã‚´ãƒªï¼ˆ0-10ï¼‰
3. old_building_flag: ç¯‰35å¹´ä»¥ä¸Šãƒ•ãƒ©ã‚°
4. old_and_large_flag: ç¯‰35å¹´ä»¥ä¸Š & 80ã¡ä»¥ä¸Šãƒ•ãƒ©ã‚°
5. old_and_rural_flag: ç¯‰35å¹´ä»¥ä¸Š & åœ°æ–¹ãƒ•ãƒ©ã‚°

ç›®æ¨™: CV MAPE 27.0%ä»¥ä¸‹ï¼ˆexp003: 27.47%ã‹ã‚‰0.5ptä»¥ä¸Šæ”¹å–„ï¼‰

å¤‰æ›´ç‚¹:
- LGBMRegressorï¼ˆsklearn APIï¼‰ã«å¤‰æ›´
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ï¼ˆæ­£å‰‡åŒ–ã€baggingã€colsampleç­‰ï¼‰
- early_stoppingè¿½åŠ 
"""

import sys
from pathlib import Path

# ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å„ªå…ˆ
current_dir = Path(__file__).resolve().parent
sys.path.insert(0, str(current_dir))

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).resolve().parents[3]
sys.path.insert(1, str(project_root / "04_src"))

import json
import mlflow
import polars as pl
from lightgbm import LGBMRegressor, early_stopping, log_evaluation
import numpy as np
import yaml
from datetime import datetime
from sklearn.model_selection import KFold

# å…±é€šã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆ04_src/ï¼‰
from data.loader import DataLoader
from features.base import set_seed
from evaluation.metrics import calculate_mape
from training.utils.mlflow_helper import (
    log_dataset_info,
    log_cv_results,
    log_feature_list,
    log_model_params,
)

# å®Ÿé¨“å›ºæœ‰ã®å‰å‡¦ç†ï¼ˆã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ï¼‰
from preprocessing import (
    preprocess_for_training,
    ALL_FEATURES,
    CATEGORICAL_FEATURES,
    TARGET_ENCODING_COLUMNS,
    COUNT_ENCODING_COLUMNS,
    NUMERIC_FEATURES,
    AGE_NUMERIC_FEATURES,
    AGE_THRESHOLD,
    AREA_THRESHOLD,
    MAJOR_CITIES,
)


# ===== è¨­å®š =====
SEED = 42
N_SPLITS = 3
EARLY_STOPPING_ROUNDS = 1000


def train_exp004():
    """exp004 age_features ã®è¨“ç·´"""

    # ã‚·ãƒ¼ãƒ‰å›ºå®š
    set_seed(SEED)

    # MLflowå®Ÿé¨“è¨­å®š
    mlflow.set_experiment("signate_mlit_rental_price")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"age_features_{timestamp}"

    with mlflow.start_run(run_name=run_name):
        print(f"ğŸš€ è¨“ç·´é–‹å§‹: {run_name}")

        # ===== ã‚¿ã‚°è¨­å®š =====
        mlflow.set_tag("experiment_type", "age_features")
        mlflow.set_tag("experiment_id", "exp004")
        mlflow.set_tag("model_family", "gbdt")
        mlflow.set_tag("status", "running")
        mlflow.set_tag("base_experiment", "exp003_baseline_v2")

        # ===== ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ =====
        print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        config_path = project_root / "03_configs" / "data.yaml"
        with open(config_path, "r", encoding="utf-8") as f:
            data_config = yaml.safe_load(f)

        # ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
        data_config["data"]["train_path"] = str(project_root / data_config["data"]["train_path"])
        data_config["data"]["test_path"] = str(project_root / data_config["data"]["test_path"])
        data_config["data"]["sample_submit_path"] = str(project_root / data_config["data"]["sample_submit_path"])

        loader = DataLoader(config=data_config, add_address_columns=False)

        train = loader.load_train()
        test = loader.load_test()

        # IDåˆ—ã‚’å…ˆã«ä¿å­˜ï¼ˆå‰å‡¦ç†å¾Œã«å‚ç…§ã™ã‚‹ãŸã‚ï¼‰
        train_ids = np.arange(len(train))
        test_ids = test["id"].to_numpy()

        print(f"  - Train: {train.shape}")
        print(f"  - Test: {test.shape}")

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¨˜éŒ²
        log_dataset_info(train, prefix="train")
        log_dataset_info(test, prefix="test")

        # ===== CVåˆ†å‰²ã‚’å…ˆã«ä½œæˆï¼ˆTargetEncodingç”¨ï¼‰ =====
        cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)
        cv_splits = list(cv.split(train))

        # ===== å‰å‡¦ç† =====
        print("\nğŸ”§ å‰å‡¦ç†ä¸­...")
        X_train, X_test, y_train = preprocess_for_training(train, test, cv_splits=cv_splits)

        print(f"\nğŸ“Š ç‰¹å¾´é‡æƒ…å ±:")
        print(f"  - ç‰¹å¾´é‡æ•°: {len(ALL_FEATURES)}")
        print(f"  - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡: {len(CATEGORICAL_FEATURES)}")
        print(f"  - æ–°è¦ç‰¹å¾´é‡ï¼ˆç¯‰å¹´æ•°é–¢é€£ï¼‰: {len(AGE_NUMERIC_FEATURES)}")

        # ç‰¹å¾´é‡æƒ…å ±ã‚’è¨˜éŒ²
        mlflow.log_param("n_features", len(ALL_FEATURES))
        mlflow.log_param("n_categorical_features", len(CATEGORICAL_FEATURES))
        mlflow.log_param("seed", SEED)
        mlflow.log_param("n_splits", N_SPLITS)
        mlflow.log_param("early_stopping_rounds", EARLY_STOPPING_ROUNDS)

        # exp004å›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        mlflow.log_param("n_age_features", len(AGE_NUMERIC_FEATURES))
        mlflow.log_param("age_threshold", AGE_THRESHOLD)
        mlflow.log_param("area_threshold", AREA_THRESHOLD)
        mlflow.log_param("major_cities", str(MAJOR_CITIES))

        # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’ä¿å­˜
        log_feature_list(ALL_FEATURES, artifact_path="features.txt")

        # ===== LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼ˆsklearn APIï¼‰ =====
        lgb_params = {
            # åŸºæœ¬è¨­å®š
            "objective": "regression",
            "metric": "mape",
            "boosting_type": "gbdt",

            # å­¦ç¿’ç‡ï¼ˆå°ã•ã‚ã«è¨­å®šã€early_stoppingã§åˆ¶å¾¡ï¼‰
            "learning_rate": 0.01,

            # æœ¨ã®æ§‹é€ 
            "max_depth": 7,
            "num_leaves": 63,  # 2^max_depth - 1 ç¨‹åº¦

            # æ­£å‰‡åŒ–
            "reg_lambda": 1.0,  # L2
            "reg_alpha": 0.1,   # L1

            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
            "colsample_bytree": 0.7,  # ç‰¹å¾´é‡ã®70%ã‚’ä½¿ç”¨
            "subsample": 0.9,         # ãƒ‡ãƒ¼ã‚¿ã®90%ã‚’ä½¿ç”¨
            "subsample_freq": 3,      # 3å›ã«1å›bagging

            # åˆ†å‰²æ¡ä»¶
            "min_child_samples": 20,

            # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆearly_stoppingã§åˆ¶å¾¡ï¼‰
            "n_estimators": 10000,

            # å†ç¾æ€§
            "random_state": SEED,
            "deterministic": True,

            # ãã®ä»–
            "importance_type": "gain",
            "verbose": -1,
            "force_row_wise": True,
        }

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²
        log_model_params(lgb_params, prefix="lgb")

        print("\nğŸ“‹ LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        print(f"  - learning_rate: {lgb_params['learning_rate']}")
        print(f"  - max_depth: {lgb_params['max_depth']}")
        print(f"  - num_leaves: {lgb_params['num_leaves']}")
        print(f"  - reg_lambda (L2): {lgb_params['reg_lambda']}")
        print(f"  - reg_alpha (L1): {lgb_params['reg_alpha']}")
        print(f"  - colsample_bytree: {lgb_params['colsample_bytree']}")
        print(f"  - subsample: {lgb_params['subsample']}")
        print(f"  - n_estimators: {lgb_params['n_estimators']} (with early_stopping)")

        # ===== ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆ3-Fold CVï¼‰ =====
        print("\nğŸ¤– 3-Fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹...")

        cv_scores = []
        oof_predictions = np.zeros(len(X_train))
        test_predictions = np.zeros(len(X_test))
        feature_importance = np.zeros(len(ALL_FEATURES))
        best_iterations = []

        # Polars â†’ pandaså¤‰æ›ï¼ˆç‰¹å¾´é‡åã‚’ä¿æŒã™ã‚‹ãŸã‚ï¼‰
        X_train_pd = X_train.to_pandas()
        X_test_pd = X_test.to_pandas()
        y_train_np = y_train.to_numpy()

        for fold_idx, (train_idx, val_idx) in enumerate(cv_splits):
            print(f"\n--- Fold {fold_idx + 1}/{N_SPLITS} ---")

            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆpandas DataFrameã§ç‰¹å¾´é‡åã‚’ä¿æŒï¼‰
            X_tr = X_train_pd.iloc[train_idx]
            X_val = X_train_pd.iloc[val_idx]
            y_tr, y_val = y_train_np[train_idx], y_train_np[val_idx]

            # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            model = LGBMRegressor(**lgb_params)

            # è¨“ç·´ï¼ˆsklearn APIï¼‰
            model.fit(
                X_tr, y_tr,
                eval_set=[(X_val, y_val)],
                eval_metric="mape",
                callbacks=[
                    early_stopping(stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=False),
                    log_evaluation(period=500),
                ],
            )

            # best_iterationå–å¾—
            best_iter = model.best_iteration_ if hasattr(model, 'best_iteration_') else model.n_estimators
            best_iterations.append(best_iter)

            # äºˆæ¸¬
            val_pred = model.predict(X_val)
            oof_predictions[val_idx] = val_pred

            # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ï¼‰
            test_pred = model.predict(X_test_pd)
            test_predictions += test_pred / N_SPLITS

            # MAPEè¨ˆç®—
            mape_score = calculate_mape(y_val, val_pred)
            cv_scores.append(mape_score)

            # ç‰¹å¾´é‡é‡è¦åº¦ã‚’è“„ç©ï¼ˆgainï¼‰
            feature_importance += model.feature_importances_ / N_SPLITS

            print(f"  Validation MAPE: {mape_score:.4f}%")
            print(f"  Best iteration: {best_iter}")

        # ===== CVçµæœã¾ã¨ã‚ =====
        print("\n" + "=" * 60)
        print("ğŸ“ˆ ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ")
        print("=" * 60)
        print(f"  å¹³å‡ MAPE: {np.mean(cv_scores):.4f}%")
        print(f"  æ¨™æº–åå·®:   {np.std(cv_scores):.4f}%")
        print(f"  æœ€å°å€¤:     {np.min(cv_scores):.4f}%")
        print(f"  æœ€å¤§å€¤:     {np.max(cv_scores):.4f}%")
        print(f"  å¹³å‡ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {np.mean(best_iterations):.0f}")
        print("=" * 60)

        # CVçµæœã‚’MLflowã«è¨˜éŒ²
        log_cv_results(np.array(cv_scores), metric_name="mape")
        mlflow.log_metric("avg_best_iteration", np.mean(best_iterations))

        # OOF MAPE
        oof_mape = calculate_mape(y_train_np, oof_predictions)
        mlflow.log_metric("oof_mape", oof_mape)
        print(f"\n  OOF MAPE: {oof_mape:.4f}%")

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path(__file__).parent.parent / "outputs"
        output_dir.mkdir(parents=True, exist_ok=True)

        # ===== OOFäºˆæ¸¬ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼åˆ†æç”¨ï¼‰ =====
        print("\nğŸ“Š OOFäºˆæ¸¬ä¿å­˜ä¸­...")
        oof_df = pl.DataFrame({
            "id": train_ids,
            "actual": y_train_np,
            "predicted": oof_predictions,
        })
        oof_path = output_dir / f"oof_predictions_{timestamp}.csv"
        oof_df.write_csv(oof_path)
        print(f"  âœ“ ä¿å­˜å®Œäº†: {oof_path}")
        mlflow.log_artifact(oof_path, artifact_path="predictions")

        # ===== ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜ =====
        print("\nğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜ä¸­...")
        importance_dict = {
            "feature": ALL_FEATURES,
            "importance": feature_importance.tolist(),
        }
        # ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’è¡¨ç¤º
        sorted_indices = np.argsort(feature_importance)[::-1]
        print("  Top 15 Features:")
        for i, idx in enumerate(sorted_indices[:15]):
            feat_name = ALL_FEATURES[idx]
            # æ–°è¦ç‰¹å¾´é‡ã«ãƒãƒ¼ã‚¯ã‚’ä»˜ã‘ã‚‹
            marker = "â­" if feat_name in AGE_NUMERIC_FEATURES else ""
            print(f"    {i+1}. {feat_name}: {feature_importance[idx]:.4f} {marker}")

        # æ–°è¦ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’åˆ¥é€”è¡¨ç¤º
        print("\n  æ–°è¦ç‰¹å¾´é‡ï¼ˆç¯‰å¹´æ•°é–¢é€£ï¼‰ã®é‡è¦åº¦:")
        for feat in AGE_NUMERIC_FEATURES:
            if feat in ALL_FEATURES:
                idx = ALL_FEATURES.index(feat)
                rank = list(sorted_indices).index(idx) + 1
                print(f"    {feat}: {feature_importance[idx]:.4f} (é †ä½: {rank}/{len(ALL_FEATURES)})")

        # JSONå½¢å¼ã§ä¿å­˜
        importance_path = output_dir / f"feature_importance_{timestamp}.json"
        with open(importance_path, "w", encoding="utf-8") as f:
            json.dump(importance_dict, f, ensure_ascii=False, indent=2)
        print(f"  âœ“ ä¿å­˜å®Œäº†: {importance_path}")
        mlflow.log_artifact(importance_path, artifact_path="feature_importance")

        # CSVå½¢å¼ã§ã‚‚ä¿å­˜ï¼ˆå¯è¦–åŒ–ã—ã‚„ã™ã„ï¼‰
        importance_df = pl.DataFrame({
            "feature": ALL_FEATURES,
            "importance": feature_importance,
        }).sort("importance", descending=True)
        importance_csv_path = output_dir / f"feature_importance_{timestamp}.csv"
        importance_df.write_csv(importance_csv_path)
        mlflow.log_artifact(importance_csv_path, artifact_path="feature_importance")

        # ===== æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ =====
        print("\nğŸ“¤ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆä¸­...")

        submission = pl.DataFrame({
            "id": test_ids,
            "money_room": test_predictions,
        })

        submission_path = output_dir / f"submission_{timestamp}.csv"
        submission.write_csv(submission_path)

        print(f"  âœ“ ä¿å­˜å®Œäº†: {submission_path}")

        # MLflowã«è¨˜éŒ²
        mlflow.log_artifact(submission_path, artifact_path="submissions")

        # ===== å®Ÿé¨“å®Œäº† =====
        mlflow.set_tag("status", "completed")

        # çµæœã‚µãƒãƒªãƒ¼
        print("\n" + "=" * 60)
        print("ğŸ“‹ å®Ÿé¨“ã‚µãƒãƒªãƒ¼ï¼ˆexp004: age_featuresï¼‰")
        print("=" * 60)
        print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆexp003ï¼‰: 27.47%")
        print(f"  ä»Šå›ã®çµæœ:              {np.mean(cv_scores):.2f}%")
        improvement = 27.47 - np.mean(cv_scores)
        if improvement > 0:
            print(f"  æ”¹å–„:                   {improvement:.2f}pt âœ…")
        else:
            print(f"  æ‚ªåŒ–:                   {-improvement:.2f}pt âŒ")
        print("=" * 60)
        print("\nâœ… è¨“ç·´å®Œäº†ï¼")


if __name__ == "__main__":
    train_exp004()
