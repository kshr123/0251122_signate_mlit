# LightGBM パラメータ解説

## 概要

LightGBMの主要パラメータについて、効果と調整指針をまとめる。

---

## 木の構造パラメータ

### max_depth（木の深さ）

根から葉までの「条件分岐の回数」を制限する。

```
深さ1:  築年数 > 20?
           ├── Yes → 予測A
           └── No  → 予測B

深さ2:  築年数 > 20?
           ├── Yes → 面積 > 50?
           │           ├── Yes → 予測A
           │           └── No  → 予測B
           └── No  → 予測C

深さ3: さらに分岐が続く...
```

| 値 | 効果 |
|---|---|
| 浅い (4-6) | 単純な条件のみ → 高速、汎化しやすい |
| 深い (8-12) | 複合条件（AかつBかつC...） → 複雑なパターン学習可能 |

**典型値**: 6〜10

---

### num_leaves（葉の数）

木全体で持てる「最終予測グループの数」を制限する。

```
num_leaves=4 の場合:

        [根]
       /    \
     [?]    [?]
    /  \      \
  葉1  葉2   [?]
            /   \
          葉3   葉4

→ 最終的に4つの予測グループ
```

| 値 | 効果 |
|---|---|
| 少ない (15-31) | 粗い分類 → シンプル、汎化しやすい |
| 多い (63-127) | 細かい分類 → 表現力高いが過学習リスク |

**典型値**: 31〜127

---

### max_depth と num_leaves の関係

完全二分木なら `葉の数 = 2^深さ`

- max_depth=8 → 最大 2^8 = 256葉
- num_leaves=63 で制限 → 63葉まで

```
LightGBMの動き:
1. max_depth の範囲内で
2. num_leaves に達するまで分岐を増やす
3. どちらか先に制限に達したら停止
```

LightGBMはleaf-wise成長（誤差が大きい葉から優先分裂）のため、num_leavesが実質的な制約になることが多い。

---

### min_child_samples（葉の最小サンプル数）

葉ノードに必要な「最低データ数」。

```
min_child_samples=50: 50件以上ないと分岐不可
min_child_samples=10: 10件でもOK
```

| 値 | 効果 |
|---|---|
| 大きい (50-100) | 大雑把な分割のみ → 安定、ノイズに強い |
| 小さい (5-20) | 少数派も捉える → 細かいパターン学習（過学習リスク） |

**典型値**: 20〜50

**不均衡データの場合**: 少数クラスを捉えるため小さめに設定することがある。

---

### min_child_weight（葉の最小ヘシアン和）

葉ノードに必要な「ヘシアンの合計値」の最小値。

```
葉のヘシアン合計 < min_child_weight → 分割禁止
```

| 値 | 効果 |
|---|---|
| 大きい (10-50) | 分割条件が厳しい → 過学習抑制 |
| 小さい (0.001-1) | 分割しやすい → 細かいパターン学習 |

**典型値**: 1〜10（回帰）、0.001〜1（分類）

#### ヘシアン（Hessian）とは

損失関数の**2階微分**（曲率）。「損失関数がどれだけ曲がっているか」を表す。

```
1階微分（勾配）= 坂の傾き     → どっちに進む？
2階微分（ヘシアン）= 坂の曲率  → どれくらい曲がってる？
```

**回帰（MSE損失）の場合**:
```
ヘシアン = 1（常に一定）
→ min_child_weight=42 なら、葉に最低42サンプル必要
→ min_child_samplesと似た効果
```

**分類（logloss）の場合**:
```
ヘシアン = p × (1 - p)  ※pは予測確率

p=0.5（半々）→ ヘシアン = 0.25（最大）→ 曲率急 → まだ学習余地あり
p=0.99（確信）→ ヘシアン = 0.01（小）→ 曲率緩 → もう最適に近い
```

#### 葉の更新式との関係

勾配ブースティングの葉の値は以下で計算される：

```
葉の値 = -Σ(勾配) / (Σ(ヘシアン) + λ)
```

ヘシアンは**分母**に入るため：
- ヘシアン大 → 更新量が小さい（慎重に更新）
- ヘシアン小 → 更新量が大きい（極端な値になりやすい）

min_child_weightは「Σ(ヘシアン)の最小値」を制限することで、
極端な更新を防ぎ、過学習を抑制する。

---

## 学習パラメータ

### learning_rate（学習率）

各木が予測を更新する「一歩の大きさ」。

```
予測値 = 前の予測 + learning_rate × 今の木の予測
```

| 値 | 効果 |
|---|---|
| 高い (0.1-0.3) | 大きく更新 → 少ない木で完了 → 速いが荒い |
| 低い (0.01-0.05) | 小さく更新 → 多くの木が必要 → 遅いが精密 |

**典型値**: 0.01〜0.1

**指針**:
- 精度重視なら低め + 多くのイテレーション
- 速度重視なら高め + 少ないイテレーション
- early_stoppingと組み合わせて自動調整

---

### n_estimators（木の本数）

作成する木の最大数。early_stoppingと組み合わせて使用。

```
n_estimators=5000, early_stopping_rounds=100:
→ 最大5000本だが、100本連続で改善なければ停止
```

**典型値**: 1000〜10000（early_stopping前提）

---

## 正則化パラメータ

### reg_alpha（L1正則化）

重みの絶対値にペナルティ。不要な特徴量の重みを0に近づける（特徴量選択効果）。

```
Loss = 元のLoss + reg_alpha × Σ|重み|
```

| 値 | 効果 |
|---|---|
| 大きい (1-10) | 多くの特徴量が無効化 → スパースなモデル |
| 小さい (0.001-0.1) | 弱い制約 → 多くの特徴量を使用 |

**典型値**: 0〜1

---

### reg_lambda（L2正則化）

重みの二乗にペナルティ。全ての重みを小さくする（滑らかにする）。

```
Loss = 元のLoss + reg_lambda × Σ(重み^2)
```

| 値 | 効果 |
|---|---|
| 大きい (1-10) | 強い制約 → 全体的に控えめな予測 |
| 小さい (0.001-0.1) | 弱い制約 → 自由にフィット |

**典型値**: 0〜1

---

## サンプリングパラメータ

### feature_fraction（特徴量サンプリング）

各木で使う特徴量の割合をランダムに選択。

```
feature_fraction=0.6:
全246特徴量のうち、毎回約148個だけ使って木を作る
→ 木ごとに違う特徴量セット → 多様性が生まれる
```

| 値 | 効果 |
|---|---|
| 1.0 | 全特徴量使用 → 最大情報だが同質的 |
| 0.5-0.8 | ランダム選択 → 過学習抑制、アンサンブル効果 |

**典型値**: 0.6〜1.0

---

### bagging_fraction / bagging_freq（データサンプリング）

データの一部だけ使って木を作る。

```
bagging_freq=3: 3本ごとにサンプリング実行
bagging_fraction=0.7: その時データの70%をランダム選択
```

| 効果 |
|---|
| データにランダム性 → 過学習抑制 |
| 外れ値の影響軽減 |
| 学習の高速化 |

**典型値**: bagging_fraction=0.7〜1.0, bagging_freq=1〜5

#### 動作の詳細

```
bagging_freq=5, bagging_fraction=0.945 の場合：

木1: 全データで学習
木2: 全データで学習
木3: 全データで学習
木4: 全データで学習
木5: 全データで学習
木6: ★ 94.5%をランダム抽出して学習（5.5%は使わない）
木7: 全データで学習
...
木11: ★ また94.5%をランダム抽出（前回とは別のサンプル）
```

#### feature_fraction との役割分担

多様性の作り方には2種類ある：

| パラメータ | 対象 | 効果 |
|-----------|------|------|
| feature_fraction | 特徴量 | 木ごとに使う特徴量を変える |
| bagging_fraction | データ | 木ごとに使うサンプルを変える |

**よくある戦略**:
- feature_fraction を低め（0.5-0.7）→ 主なランダム性
- bagging_fraction を高め（0.9-1.0）→ 情報量確保、補助的なランダム性

---

## 分類専用パラメータ

### is_unbalance

クラス不均衡時に自動で重み調整。

```
正例20%、負例80%の場合:
→ 正例の重みを自動で4倍に調整
```

**使用場面**: 不均衡データ（片方のクラスが少ない）

---

### scale_pos_weight

正例の重みを手動指定。

```
scale_pos_weight = 負例数 / 正例数
```

is_unbalanceの代わりに手動で細かく調整したい場合に使用。

---

## 葉の出力と予測

### 分類の場合

各葉は**log-odds（対数オッズ）**を出力する（確率ではない）。

```
【1本の木の葉】
葉1: +0.3
葉2: -0.5
葉3: +0.1

【全ての木を足し合わせ】
スコア = 木1の葉 + 木2の葉 + 木3の葉 + ...
       = 0.3 + 0.2 + (-0.1) + ... = 2.5

【最後にsigmoidで確率に変換】
確率 = 1 / (1 + exp(-スコア)) = 1 / (1 + exp(-2.5)) ≈ 0.924
```

- 正のスコア → 正例寄り（確率 > 0.5）
- 負のスコア → 負例寄り（確率 < 0.5）

---

## チューニングの指針

### 過学習している場合

- num_leaves を減らす
- min_child_samples を増やす
- reg_alpha / reg_lambda を増やす
- feature_fraction / bagging_fraction を減らす

### 学習不足の場合

- num_leaves を増やす
- max_depth を増やす
- min_child_samples を減らす
- learning_rate を下げて n_estimators を増やす

### 不均衡データの場合

- is_unbalance=True または scale_pos_weight を設定
- min_child_samples を小さめに（少数クラスを捉える）

---

## 参考: exp019チューニング結果

### 分類器（低価格判定）

```yaml
# ベースライン → チューニング後
learning_rate: 0.05 → 0.018    # より精密に
num_leaves: 63 → 125           # 表現力向上
max_depth: 8 → 10              # 深い条件分岐
min_child_samples: 50 → 10     # 細かいパターン捕捉
reg_alpha: 0.1 → 0.047         # やや緩和
reg_lambda: 0.1 → 0.097        # ほぼ同じ
feature_fraction: - → 0.59     # ランダム性追加
bagging_fraction: - → 0.73     # ランダム性追加
bagging_freq: - → 3
```

結果: logloss=0.160, PR-AUC=0.927

### 回帰器（低価格専用）

```yaml
# ベースライン → チューニング後
learning_rate: 0.03 → 0.032    # ほぼ同じ
num_leaves: 63 → 110           # 表現力向上（+74%）
max_depth: 10 → 11             # やや深く
min_child_samples: 100 → 107   # ほぼ同じ
min_child_weight: 10.0 → 42.3  # 大幅増（+323%）→ 分割条件厳しく
reg_alpha: 1.0 → 0.092         # 大幅減（-91%）→ L1緩和
reg_lambda: 1.0 → 3.87         # 大幅増（+287%）→ L2強化
feature_fraction: 0.8 → 0.59   # 減少（-26%）→ ランダム性追加
bagging_fraction: 0.8 → 0.95   # 増加（+18%）→ 情報量確保
bagging_freq: 1 → 5
```

結果: MAPE=13.14%, RMSE=1.51M

**解釈**: 表現力(num_leaves↑) × 過学習抑制(reg_lambda↑, min_child_weight↑, feature_fraction↓)のバランス
